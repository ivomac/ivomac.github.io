<!DOCTYPE html>
<html>

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Ivo Maceira LLM Experiments</title>
  <link rel="stylesheet" href="/assets/css/style.css" type="text/css">
</head>

<body>
  <script src="/assets/js/nav.js"></script>

  <main>
    <h2><a href="https://docs.google.com/document/d/1LbG9zG2ppu3uZNk0rkMUZ__FLBk6dpoqeuEPJC23mAo">ðŸ“Š LLM Experiments</a>
    </h2>

    <p>
      In this project I studied the effect of telling LLMs to not answer a question if they don't
      know the answer, when we ask the LLM questions on obscure facts. I analyse the effects of the prompt on accuracy,
      changes in the fraction of "correct", "incorrect", and "don't know" answers.
    </p>
    <p>
      The results show that most models are more likely to say "I don't know" when they had previously
      answered incorrectly, resulting in higher accuracy. Some model dependence is observed that seems dependent on
      whether the model "reasons", with reasoning models saying "I don't know" less often.
    </p>
    <p>
      The mini-paper can be found in <a
        href="https://docs.google.com/document/d/1LbG9zG2ppu3uZNk0rkMUZ__FLBk6dpoqeuEPJC23mAo">this Google doc</a>,
      while the source code is on <a href="https://github.com/ivomac/mechanistic-interpretability">Github</a>.
    </p>
    <div class="plot-container">
      <img src="/assets/images/projects/mech_interp/analysis.png">
    </div>
  </main>

</body>

</html>
